# AI Writer - Local Inference App

A beautiful, modern AI writing assistant that runs entirely on your local machine using Ollama. No cloud APIs, no data sharing - everything stays private and local.

![AI Writer App](https://img.shields.io/badge/AI-Writer-blue) ![Ollama](https://img.shields.io/badge/Powered%20by-Ollama-green) ![Local](https://img.shields.io/badge/100%25-Local-red) ![Privacy](https://img.shields.io/badge/Privacy-First-orange)

## ğŸ¯ App Type Selected: AI Writer

This app can generate three types of content:
- **ğŸ“– Stories**: Creative short stories and narratives
- **âœï¸ Blog Intros**: Professional blog introductions
- **ğŸ¦ Tweets**: Engaging social media posts

## âœ¨ Features

### Core Features
- âœ… **Prompt Input Box**: Clean, expandable textarea for your ideas
- âœ… **Model Output Display**: Beautiful formatted output with syntax highlighting
- âœ… **Temperature Setting**: Adjustable creativity slider (0.1 - 1.5)
- âœ… **Loading UI**: Elegant spinner with generation status
- âœ… **Output Logging**: Local file logging (`outputs.log`) + frontend history

### Advanced Features
- ğŸ§  **Multiple Models**: Support for all Gemma 3 variants (1B, 4B, 12B, 27B)
- ğŸ¨ **Content Types**: Specialized prompts for stories, blog posts, and tweets
- ğŸ“± **Responsive Design**: Works perfectly on desktop, tablet, and mobile
- ğŸ”„ **Real-time Status**: Live Ollama connection monitoring
- ğŸ“‹ **Copy to Clipboard**: One-click copying of generated content
- ğŸ”„ **Regenerate**: Easy regeneration with same settings
- ğŸ“š **History**: Local browser storage of recent generations
- ğŸ¯ **Smart Placeholders**: Context-aware input suggestions
- ğŸŒˆ **Beautiful UI**: Modern glassmorphism design with smooth animations

## ğŸš€ How to Run the App Locally

### Prerequisites

1. **Install Node.js** (v16 or higher)
   - Download from [nodejs.org](https://nodejs.org/)

2. **Install Ollama**
   - Download from [ollama.ai](https://ollama.ai/)
   - Make sure it's running on port 11434

3. **Download a Model**
   ```bash
   # Recommended model (best balance of speed/quality)
   ollama pull gemma3:4b
   
   # Alternative models
   ollama pull gemma3:1b    # Faster, smaller
   ollama pull gemma3:12b   # Better quality
   ollama pull gemma3:27b   # Best quality (requires more RAM)
   ```

### Setup Instructions

1. **Clone or download this repository**
   ```bash
   git clone <your-repo-url>
   cd ai-writer-local
   ```

2. **Install dependencies**
   ```bash
   npm install
   ```

3. **Start Ollama** (if not already running)
   ```bash
   ollama serve
   ```

4. **Run the application**
   ```bash
   npm start
   ```

5. **Open your browser**
   - Navigate to `http://localhost:3000`
   - The app will automatically check Ollama connection status

### Development Mode

For development with auto-reload:
```bash
npm run dev
```

## ğŸ¤– Model Used

**Primary Model**: Gemma 3 4B (`gemma3:4b`)
- **Size**: 3.3GB
- **Context Window**: 128K tokens
- **Type**: Multimodal (Text + Image)
- **Performance**: Excellent balance of speed and quality

**Supported Models**:
- `gemma3:1b` - Fast, lightweight (815MB)
- `gemma3:4b` - Recommended (3.3GB)
- `gemma3:12b` - High quality (8.1GB)
- `gemma3:27b` - Best quality (17GB)

The app automatically detects available models and populates the dropdown.

## ğŸ® How to Use

1. **Select Content Type**
   - Choose between Story, Blog Intro, or Tweet
   - Each type has specialized prompts for better results

2. **Choose Your Model**
   - Select based on your hardware and quality needs
   - 4B is recommended for most users

3. **Adjust Creativity**
   - Use the temperature slider to control randomness
   - Lower values = more focused and conservative
   - Higher values = more creative and varied

4. **Enter Your Prompt**
   - Type your topic or idea in the input box
   - Use Ctrl+Enter for quick generation

5. **Generate Content**
   - Click Generate or use Ctrl+Enter
   - Watch the beautiful loading animation
   - Copy or regenerate as needed

6. **Browse History**
   - View your recent generations
   - Click any history item to reload it

## ğŸ“ Project Structure

```
ai-writer-local/
â”œâ”€â”€ package.json          # Dependencies and scripts
â”œâ”€â”€ server.js             # Express server with Ollama integration
â”œâ”€â”€ outputs.log           # Local generation logging (created automatically)
â”œâ”€â”€ README.md             # This file
â””â”€â”€ public/
    â”œâ”€â”€ index.html        # Main HTML file
    â”œâ”€â”€ styles.css        # Beautiful CSS with animations
    â””â”€â”€ script.js         # JavaScript for all interactions
```

## ğŸ”§ API Endpoints

- `GET /` - Serve the main application
- `POST /api/generate` - Generate content with Ollama
- `GET /api/models` - Get available Ollama models
- `GET /api/health` - Check Ollama connection status

## ğŸ¨ Design Features

- **Glassmorphism UI**: Modern transparent design with blur effects
- **Gradient Backgrounds**: Beautiful purple-blue gradients
- **Smooth Animations**: Fade-ins, hover effects, and transitions
- **Responsive Layout**: Perfect on all screen sizes
- **Dark/Light Accents**: Careful color balance for readability
- **Loading States**: Engaging spinners and status indicators

## ğŸ“ Output Logging

The app logs all generations in two ways:

1. **Local File**: `outputs.log` (JSON format for analysis)
2. **Browser Storage**: Recent generations with click-to-reload

Log entry format:
```json
{
  "timestamp": "2024-01-15T10:30:00.000Z",
  "prompt": "A magical forest adventure",
  "output": "Generated story content...",
  "type": "story",
  "model": "gemma3:4b",
  "temperature": 0.7
}
```

## ğŸ”’ Privacy & Security

- âœ… **100% Local**: No data leaves your machine
- âœ… **No API Keys**: No external services required
- âœ… **Private Storage**: All history stored locally
- âœ… **Open Source**: Full transparency in code

## ğŸ› Troubleshooting

### Ollama Connection Issues
- Ensure Ollama is installed and running
- Check that port 11434 is available
- Restart Ollama service if needed

### Model Not Found
- Pull the model using `ollama pull gemma3:4b`
- Check available models with `ollama list`

### Slow Generation
- Try a smaller model (gemma3:1b)
- Lower the temperature setting
- Ensure sufficient RAM/VRAM

## ğŸš€ Performance Tips

- **RAM Requirements**:
  - 1B model: 1GB RAM
  - 4B model: 4GB RAM
  - 12B model: 8GB RAM
  - 27B model: 16GB RAM

- **Speed Optimization**:
  - Use SSD storage for better I/O
  - Close other applications to free RAM
  - Use GPU acceleration if available

## ğŸ“„ License

MIT License - feel free to modify and distribute!

## ğŸ¤ Contributing

Contributions welcome! Please feel free to submit issues and enhancement requests.

---

**Built with â¤ï¸ using Ollama for local AI inference** 